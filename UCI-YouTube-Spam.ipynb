{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peter Kim, UCI Data Analytics Bootcamp\n",
    "January 22, 2019\n",
    "\n",
    "## Outline\n",
    "1. Files and Setup\n",
    "<br>\n",
    "\n",
    "    1. Jupyter Notebook: \"UCI - YouTube Spam.ipynb\"\n",
    "        1. CSV file: \"Youtube04-Eminem.csv\"\n",
    "    1. Make sure installed: (I'm using Anaconda distribution of Python 3)\n",
    "        1. sklearn\n",
    "        1. pandas\n",
    "        1. re\n",
    "        1. nltk\n",
    "        1. matplotlib\n",
    "        1. numpy\n",
    "1. Background and context (depending on pace)\n",
    "    1. Python\n",
    "    1. Jupyter Notebook\n",
    "    1. Scikit-Learn\n",
    "    1. Machine Learning\n",
    "    1. Text Classification\n",
    "\n",
    "\n",
    "## Text Classification with Scikit-Learn\n",
    "\n",
    "YouTube Spam Collection\n",
    "\n",
    "The YouTube Spam Collection v. 1 is a public set of YouTube comments that have been collected for spam research. It has five datasets composed by 1,956 real and non-encoded messages that were labeled as legitimate (ham) or spam.\n",
    ">Alberto, T.C., Lochter J.V., Almeida, T.A. TubeSpam: Comment Spam Filtering on YouTube. Proceedings of the 14th IEEE International Conference on Machine Learning and Applications (ICMLA'15), 1-6, Miami, FL, USA, December, 2015. (preprint).  \n",
    ">\n",
    ">Available at UCI Machine Learning Repository.\n",
    "><br>Source: http://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection#\n",
    "\n",
    "Download the zip files.  Make sure to include file \"Youtube04-Eminem.csv\" in same directory.\n",
    "\n",
    "## Great tutorial on NLP using Scikit-Learn by Kevin Markham\n",
    "* https://github.com/justmarkham/pycon-2016-tutorial \n",
    "* https://www.youtube.com/watch?v=WHocRqT-KkU\n",
    "\n",
    "## Also review the Andreas Mueller book for examples\n",
    "* http://shop.oreilly.com/product/0636920030515.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/peter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with Eminem YouTube comments\n",
    "df_eminem = pd.read_csv(\"Youtube04-Eminem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eminem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eminem.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine dataframe \n",
    "{Class 0: ham, Class 1: spam}\n",
    "<br>'CONTENT' field is the comment, 'CONTENT' field is the class.\n",
    "<br>Ignore the other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12rwfnyyrbsefonb232i5ehdxzkjzjs2</td>\n",
       "      <td>Lisa Wellas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+447935454150 lovely girl talk to me xxx﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04</td>\n",
       "      <td>jason graham</td>\n",
       "      <td>2015-05-29T02:26:10.652000</td>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z13vsfqirtavjvu0t22ezrgzyorwxhpf3</td>\n",
       "      <td>Ajkal Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12wjzc4eprnvja4304cgbbizuved35wxcs</td>\n",
       "      <td>Dakota Taylor</td>\n",
       "      <td>2015-05-29T02:13:07.810000</td>\n",
       "      <td>Cool﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13xjfr42z3uxdz2223gx5rrzs3dt5hna</td>\n",
       "      <td>Jihad Naser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello I&amp;#39;am from Palastine﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            COMMENT_ID         AUTHOR  \\\n",
       "0    z12rwfnyyrbsefonb232i5ehdxzkjzjs2    Lisa Wellas   \n",
       "1  z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04   jason graham   \n",
       "2    z13vsfqirtavjvu0t22ezrgzyorwxhpf3     Ajkal Khan   \n",
       "3  z12wjzc4eprnvja4304cgbbizuved35wxcs  Dakota Taylor   \n",
       "4    z13xjfr42z3uxdz2223gx5rrzs3dt5hna    Jihad Naser   \n",
       "\n",
       "                         DATE  \\\n",
       "0                         NaN   \n",
       "1  2015-05-29T02:26:10.652000   \n",
       "2                         NaN   \n",
       "3  2015-05-29T02:13:07.810000   \n",
       "4                         NaN   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0          +447935454150 lovely girl talk to me xxx﻿      1  \n",
       "1    I always end up coming back to this song<br />﻿      0  \n",
       "2  my sister just received over 6,500 new <a rel=...      1  \n",
       "3                                              Cool﻿      0  \n",
       "4                     Hello I&#39;am from Palastine﻿      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine dataframe  {Class 0: ham, Class 1: spam}\n",
    "df_eminem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    245\n",
       "0    203\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eminem['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ham     203\n",
       "Spam    245\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_eminem['CLASS'].value_counts()\n",
    "# df_eminem['CLASS'].value_counts().rename(index={1: 'Spam', 0: 'Ham'})\n",
    "vc_class = df_eminem['CLASS'].value_counts()\n",
    "vc_class = vc_class.rename(index={1: 'Spam', 0: 'Ham'})\n",
    "vc_class = vc_class.sort_index()\n",
    "vc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3f7a5b2160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hist(bins=bins, grid=False); plt.xticks([0,1])\n",
    "vc_class.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Content and Classes into train and test sets (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_eminem['CONTENT'], df_eminem['CLASS'], \n",
    "                                                    random_state=1, test_size=0.2, stratify=df_eminem['CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training size:  (358,)\n",
      "y training size:  (358,)\n",
      "X test size:  (90,)\n",
      "y test size:  (90,)\n"
     ]
    }
   ],
   "source": [
    "print('X training size: ', X_train.shape)\n",
    "print('y training size: ', y_train.shape)\n",
    "print('X test size: ', X_test.shape)\n",
    "print('y test size: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Text into DTM (with Pre-Processing)\n",
    "* Pre-processing function\n",
    "* List comprehension with pre-processing function\n",
    "* LemmaTokenizer class\n",
    "* CountVectorizer object with LemmaTokenizer\n",
    "\n",
    "# Surprise: performs worse with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre-processing from Kaggle tutorial\n",
    "\n",
    "# stop_nltk = stopwords.words(\"english\")\n",
    "# stop_nltk_plus = stop_nltk + [u'a',u'b',u'c',u'd',u'e',u'f',u'g',u'h',u'i',u'j',\n",
    "#                          u'k',u'l',u'm',u'n',u'o',u'p',u'q',u'r',u's',u't',\n",
    "#                          u'u',u'v',u'w',u'x',u'y',u'z']\n",
    "# # 4. In Python, searching a set is much faster than searching\n",
    "# #     a list, so convert the stop words to a set\n",
    "# stops = set(stop_nltk_plus)\n",
    "    \n",
    "# # function to process 10k documents\n",
    "# def process_10kchars(input_text):\n",
    "#     # 1. Remove non-letters, and make lowercase\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
    "        \n",
    "#     # 3. Convert to lower case, split into individual words\n",
    "#     words = letters_only.lower().split()    \n",
    "    \n",
    "#     # 5. Remove stop words\n",
    "#     meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "#     # 6. Join the words back into one string separated by space, \n",
    "#     # and return the result.\n",
    "#     return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Wall time: 22.1 s\n",
    "\n",
    "# # list comprehension on X_train_chunks, with process_10kchars\n",
    "# X_train_process = [process_10kchars(text_file) for text_file in X_train]\n",
    "\n",
    "# X_test_process = [process_10kchars(text_file) for text_file in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a new CountVectorizer with lemmatizer\n",
    "# class LemmaTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.wnl = WordNetLemmatizer()\n",
    "#     def __call__(self, doc):\n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "# count_vect_lemma = CountVectorizer(tokenizer=LemmaTokenizer())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate CountVectorizer object, with LemmaTokenizer()\n",
    "# count_vect_lemma = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 2), max_features=1000, \n",
    "#                                    max_df=0.90, stop_words='english') \n",
    "\n",
    "# # Fit the vectorizer object to the X_train text\n",
    "# X_train_vect = count_vect_lemma.fit(X_train_process, y_train)\n",
    "\n",
    "# # Transform the training text into a document-term-matrix\n",
    "# X_train_dtm = X_train_vect.transform(X_train_process)\n",
    "\n",
    "# print \"Size of training dtm: \", X_train_dtm.shape\n",
    "\n",
    "# # Transform the test text into a document-term-matrix (input fed into models)\n",
    "# X_test_dtm = X_train_vect.transform(X_test_process)\n",
    "\n",
    "# print \"Size of test dtm: \", X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize text into DTM (no pre-processing)\n",
    "* Leave text unprocessed, no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dtm:  (358, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a vectorizer for document-term-matrix\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit the vectorizer object to the X_train text\n",
    "X_train_vect = vect.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training text into a document-term-matrix\n",
    "X_train_dtm = X_train_vect.transform(X_train)\n",
    "\n",
    "print(\"Size of training dtm: \", X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '047000', '09', '10', '100', '1000', '100877300245414', '11', '12', '12year', '1337', '14', '15', '16', '17', '17yr', '18', '19', '1990']\n",
      "\n",
      "\n",
      "['admitting', 'adult', 'advance', 'advertise', 'advertisements', 'afraid', 'aftermath', 'again', 'ago', 'agree', 'agreeable', 'aimbwbfqbzg', 'album', 'alcoholic', 'ali', 'alive', 'all', 'almost', 'aloidia', 'already']\n",
      "\n",
      "\n",
      "['yahoo', 'yeah', 'year', 'years', 'yesterday', 'yfuy4gkr1c', 'yo', 'yoffa', 'you', 'young', 'your', 'yourself', 'youtu', 'youtube', 'youtuber', 'youtubers', 'yrs', 'ytma', 'yuttx04oyqq', 'zonepa']\n"
     ]
    }
   ],
   "source": [
    "# The vectorizer object has method .get_feature_names()\n",
    "# to show the words and bi-grams used in feature vector.  \n",
    "print(vect.get_feature_names()[0:20])\n",
    "print(\"\\n\")\n",
    "print(vect.get_feature_names()[100:120])\n",
    "print(\"\\n\")\n",
    "print(vect.get_feature_names()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the actual dtm look like?  Convert to array (from sparse matrix).\n",
    "# Just one line (i.e. one document).  \n",
    "X_train_dtm.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dtm:  (90, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Transform the test text into a document-term-matrix (input fed into models)\n",
    "X_test_dtm = X_train_vect.transform(X_test)\n",
    "\n",
    "print(\"Size of test dtm: \", X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example Without GridSearchCV\n",
    "* In practice, the GridSearchCV class accomplishes same result, while tuning combinations of model parameters.\n",
    "* But to make it simpler to follow the workflow, here is an example of text classification step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Instatiate a classifier object.\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Step 2: \"Fit\" training data onto model, both data and labels.\n",
    "# The machine is \"learning\" how the training words match the label data (or \"classes\").  \n",
    "rf_clf.fit(X_train_dtm, y_train)\n",
    "\n",
    "# Step 3: Predict test data using model, only data (not labels)\n",
    "# Store results as predictions on test data ... next we will compare with real labels.  \n",
    "rf_test_predictions = rf_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: \n",
      "0.9666666666666667\n",
      "\n",
      "Confusion Matrix: \n",
      "(rows are actual, columns are predictions)\n",
      "[[40  1]\n",
      " [ 2 47]]\n",
      "\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.95      0.98      0.96        41\n",
      "       Spam       0.98      0.96      0.97        49\n",
      "\n",
      "avg / total       0.97      0.97      0.97        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Classifier Accuracy: \")\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(\"(rows are actual, columns are predictions)\")\n",
    "print(metrics.confusion_matrix(y_test, rf_test_predictions))\n",
    "\n",
    "# print the Classification Report\n",
    "print(\"\\nClassification Report: \")\n",
    "print(metrics.classification_report(y_test, rf_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9441340782122905\n",
      "bootstrap: False\n",
      "class_weight: None\n",
      "n_estimators: 100\n",
      "CPU times: user 200 ms, sys: 48 ms, total: 248 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.95251396648\n",
    "# bootstrap: False\n",
    "# class_weight: 'balanced'\n",
    "# n_estimators: 50\n",
    "# Wall time: 40.1 s\n",
    "\n",
    "# parameters \n",
    "parameters_rf = {'n_estimators': (10, 50, 100),                 # default 10\n",
    "                 'bootstrap': (True, False),                          # default true\n",
    "                  'class_weight': ('balanced', None)}                # default None\n",
    "\n",
    "# instantiate a classifier object\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_rf = GridSearchCV(rf, parameters_rf, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_rf = gs_rf.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_rf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_rf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier: \n",
      "0.9444444444444444\n",
      "[[39  2]\n",
      " [ 3 46]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.93      0.95      0.94        41\n",
      "       Spam       0.96      0.94      0.95        49\n",
      "\n",
      "avg / total       0.94      0.94      0.94        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Classifier: \")\n",
    "\n",
    "# predict classification\n",
    "gs_rf_test_predictions = gs_rf.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_rf_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_rf_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Target Class Label Counts to Classification Report\n",
      "0: Ham, 1: Spam\n",
      "\n",
      "0    41\n",
      "1    49\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify classes against the y_test data\n",
    "print(\"Compare Target Class Label Counts to Classification Report\")\n",
    "print(\"0: Ham, 1: Spam\\n\")\n",
    "print(y_test.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification Technique #2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for MultinomialNB\n",
      "Accuracy:  0.9134078212290503\n",
      "alpha:  0.1\n",
      "fit_prior:  False\n",
      "CPU times: user 64 ms, sys: 52 ms, total: 116 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# parameters for MultinomialNB\n",
    "parameters_mnb = {'alpha': (0.001, 0.01, 0.1, 1.0, 10.0, 100.0),\n",
    "                 'fit_prior': (True, False)}    # default is True, use uniform if False\n",
    "\n",
    "# instantiate a MultinomialNB object\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_mnb = GridSearchCV(mnb, parameters_mnb, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_mnb = gs_mnb.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(\"Grid Search for MultinomialNB\")\n",
    "print(\"Accuracy: \", gs_mnb.best_score_)\n",
    "print(\"alpha: \", gs_mnb.best_params_['alpha'])\n",
    "print(\"fit_prior: \", gs_mnb.best_params_['fit_prior'])\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: \n",
      "0.9333333333333333\n",
      "[[37  4]\n",
      " [ 2 47]]\n",
      "0.9736187157789945\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.95      0.90      0.92        41\n",
      "       Spam       0.92      0.96      0.94        49\n",
      "\n",
      "avg / total       0.93      0.93      0.93        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes: \")\n",
    "\n",
    "# predict classification\n",
    "gs_mnb_test_predictions = gs_mnb.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_mnb_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_mnb_test_predictions))\n",
    "\n",
    "# calculate predicted probabilities (poorly calibrated)\n",
    "gs_y_pred_test = gs_mnb.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "# print AUC\n",
    "print(metrics.roc_auc_score(y_test, gs_y_pred_test))\n",
    "\n",
    "# print classification report\n",
    "print(metrics.classification_report(y_test, gs_mnb_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Target Class Label Counts to Classification Report\n",
      "0: Ham, 1: Spam\n",
      "\n",
      "0    41\n",
      "1    49\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify classes against the y_test data\n",
    "print(\"Compare Target Class Label Counts to Classification Report\")\n",
    "print(\"0: Ham, 1: Spam\\n\")\n",
    "print(y_test.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #3: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9497206703910615\n",
      "alpha: 0.01\n",
      "class_weight: 'balanced'\n",
      "penalty: 'elasticnet'\n",
      "CPU times: user 132 ms, sys: 60 ms, total: 192 ms\n",
      "Wall time: 301 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.949720670391\n",
    "# alpha: 0.01\n",
    "# class_weight: 'balanced'\n",
    "# penalty: 'elasticnet'\n",
    "# Wall time: 23 s\n",
    "\n",
    "# parameters for SVM\n",
    "parameters_svm = {'penalty': (None, 'l1', 'l2', 'elasticnet'),  # default is 'l2'\n",
    "                  'alpha': (0.0001, 0.01, 1.0),                 # default 0.0001\n",
    "                  'class_weight': ('balanced', None)}           # default None\n",
    "\n",
    "# instantiate a SVM object\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_svm = GridSearchCV(svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_svm = gs_svm.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_svm.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_svm.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines: \n",
      "0.9444444444444444\n",
      "[[39  2]\n",
      " [ 3 46]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.93      0.95      0.94        41\n",
      "       Spam       0.96      0.94      0.95        49\n",
      "\n",
      "avg / total       0.94      0.94      0.94        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machines: \")\n",
    "\n",
    "# predict classification\n",
    "gs_svm_test_predictions = gs_svm.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_svm_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_svm_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_svm_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Target Class Label Counts to Classification Report\n",
      "0: Ham, 1: Spam\n",
      "\n",
      "0    41\n",
      "1    49\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify classes against the y_test data\n",
    "print(\"Compare Target Class Label Counts to Classification Report\")\n",
    "print(\"0: Ham, 1: Spam\\n\")\n",
    "print(y_test.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9553072625698324\n",
      "C: 1.0\n",
      "class_weight: 'balanced'\n",
      "penalty: 'l2'\n",
      "CPU times: user 96 ms, sys: 44 ms, total: 140 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.95530726257\n",
    "# C: 1.0\n",
    "# class_weight: None\n",
    "# penalty: 'l2'\n",
    "# Wall time: 5.55 s\n",
    "\n",
    "# parameters \n",
    "parameters_lr = {'penalty': ('l1', 'l2'),               # default is 'l2'\n",
    "                  'C': (0.01, 1.0, 10.0, 100.0),            # default 1.0\n",
    "                  'class_weight': ('balanced', None)}                         # default None\n",
    "\n",
    "# instantiate a LogisticRegression object\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_lr = GridSearchCV(lr, parameters_lr, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_lr = gs_lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_lr.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_lr.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_lr.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "0.9666666666666667\n",
      "[[41  0]\n",
      " [ 3 46]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.93      1.00      0.96        41\n",
      "       Spam       1.00      0.94      0.97        49\n",
      "\n",
      "avg / total       0.97      0.97      0.97        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: \")\n",
    "\n",
    "# predict classification\n",
    "gs_lr_test_predictions = gs_lr.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_lr_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_lr_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_lr_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Target Class Label Counts to Classification Report\n",
      "0: Ham, 1: Spam\n",
      "\n",
      "0    41\n",
      "1    49\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify classes against the y_test data\n",
    "print(\"Compare Target Class Label Counts to Classification Report\")\n",
    "print(\"0: Ham, 1: Spam\\n\")\n",
    "print(y_test.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison of Models (w/o Pre-Processing, with GridSearch)\n",
    "\n",
    "(Precision, Recall, F1-Score for the Spam Class Only)\n",
    "\n",
    "| Model                    | Accuracy   | Precision    | Recall    | F1-Score    |\n",
    "|--------------------------|------------|--------------|-----------|-------------|\n",
    "| Random Forest            | 0.9555     | 0.98         | 0.94      | 0.96        |\n",
    "| Multinomial Naive Bayes  | 0.9333     | 0.92         | 0.96      | 0.94        |\n",
    "| Support Vector Machines  | 0.9778     | 1.00         | 0.96      | 0.98        |\n",
    "| Logistic Regression      | 0.9667     | 1.00         | 0.94      | 0.97        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
